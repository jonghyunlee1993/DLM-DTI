{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d565340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonghyunlee1993\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20221019_124337-1f8l7us0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jonghyunlee1993/DistilledProtBert/runs/1f8l7us0\" target=\"_blank\">XProtBert_lr-3e-05_prot_1024</a></strong> to <a href=\"https://wandb.ai/jonghyunlee1993/DistilledProtBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NAME = \"XProtBert\"\n",
    "LEARNING_RATE = 3e-5\n",
    "PROT_MAX_LEN = 1024\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, AutoModel, BertConfig, BertModel, BertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name=f'{PROJECT_NAME}_lr-{LEARNING_RATE}_prot_{PROT_MAX_LEN}',\n",
    "                           project='DistilledProtBert')\n",
    "\n",
    "# prot_seq = pd.read_csv(\"data/mol_trans/protein_sequences.csv\")\n",
    "with open(\"data/fasta_list.pkl\", \"rb\") as f:\n",
    "    fasta_list = pickle.load(f)\n",
    "\n",
    "print(len(fasta_list))\n",
    "    \n",
    "train_data, test_data = train_test_split(fasta_list, test_size=0.1, random_state=42, shuffle=True)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=5000, random_state=42, shuffle=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "teacher_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=PROT_MAX_LEN + 2,\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "class DistilledProtBert(nn.Module):\n",
    "    def __init__(self, model, hidden_dim, target_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.proj = nn.Linear(hidden_dim, target_dim, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, **batch):\n",
    "        x = self.model.base_model(input_ids=batch[\"input_ids\"], \n",
    "                                  token_type_ids=batch['token_type_ids'], \n",
    "                                  attention_mask=batch['attention_mask'])\n",
    "        x = x['last_hidden_state']\n",
    "        logits = self.proj(x)\n",
    "\n",
    "        mlm_out = self.model.cls(x)\n",
    "        pred = torch.argmax(F.softmax(mlm_out, dim=-1), dim=-1)\n",
    "        label = batch['labels']\n",
    "        masked_index = label.gt(0)\n",
    "        \n",
    "        mlm_loss = F.cross_entropy(mlm_out.reshape(-1, self.vocab_size), label.reshape(-1))\n",
    "        acc = accuracy(torch.masked_select(pred, masked_index), torch.masked_select(label, masked_index))\n",
    "        \n",
    "        return logits, mlm_loss, acc\n",
    "\n",
    "student_base = BertForMaskedLM(config)\n",
    "student_model = DistilledProtBert(student_base, hidden_dim=128, target_dim=1024, vocab_size=tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d520d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.load_state_dict(torch.load(\"weights/DistilledProtBert/test.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f1820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def encode(self, seq):\n",
    "        return self.tokenizer(\" \".join(seq), max_length=self.max_len, truncation=True)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encode(self.data[idx])\n",
    "    \n",
    "    \n",
    "def collate_batch(batch):\n",
    "    out = []\n",
    "    for b in batch:\n",
    "        out.append(b)\n",
    "        \n",
    "    return tokenizer.pad(out, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "data_sampler = RandomSampler(train_data, replacement=True, num_samples=100000)\n",
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.3)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, collate_fn=mlm_collator,\n",
    "                              num_workers=16, pin_memory=True, prefetch_factor=2, \n",
    "                              drop_last=True, sampler=data_sampler)\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(valid_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=2, collate_fn=mlm_collator)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, num_workers=16, \n",
    "                             pin_memory=True, prefetch_factor=2, collate_fn=mlm_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1282f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a796dbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_input = tokenizer(\" \".join(train_data[0]), return_tensors=\"pt\")\n",
    "student_model(**sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ed5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.3)\n",
    "test_dataset = CustomDataset(test_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, num_workers=16, \n",
    "                             pin_memory=False, prefetch_factor=2, collate_fn=mlm_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3e5ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-17.7812,  -7.3981,  -7.3770,  ...,  -6.8732,  -6.9766,  -7.1213],\n",
      "         [ -9.2509,  -3.1022,  -3.3210,  ...,  -2.8256,  -2.9352,  -3.1158],\n",
      "         [-12.7206,  -5.3841,  -5.3178,  ...,  -4.8874,  -4.9733,  -5.2469],\n",
      "         ...,\n",
      "         [-15.9112,  -6.2360,  -5.7352,  ...,  -5.7123,  -5.9387,  -5.8965],\n",
      "         [-15.8572,  -6.2203,  -5.7444,  ...,  -5.6790,  -5.9199,  -5.8770],\n",
      "         [-17.6861,  -7.3433,  -7.3187,  ...,  -6.7905,  -6.8923,  -7.0513]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    x = model.bert(input_ids=batch[\"input_ids\"], \n",
    "                      token_type_ids=batch['token_type_ids'], \n",
    "                      attention_mask=batch['attention_mask'])\n",
    "    out = model.cls(x['last_hidden_state'])\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61469fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(**batch)['logits']\n",
    "pred = torch.argmax(F.softmax(logits, dim=-1), dim=-1)\n",
    "\n",
    "label = batch['labels']\n",
    "valid_index = label.gt(0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(torch.masked_select(label, valid_index), torch.masked_select(pred, valid_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "689c54d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.0133e-04, -4.2184e-04,  2.0127e-04,  ..., -1.8263e-04,\n",
       "            5.2682e-04, -2.7157e-04],\n",
       "          [-1.5056e-04, -8.5584e-04,  4.0688e-04,  ..., -2.0433e-04,\n",
       "            2.3460e-04,  1.3156e-04],\n",
       "          [-3.1432e-04, -5.3086e-04,  2.5640e-04,  ..., -2.3224e-04,\n",
       "            3.3249e-04, -9.1479e-05],\n",
       "          ...,\n",
       "          [-4.3508e-05, -5.2341e-04,  2.4741e-04,  ..., -1.7559e-04,\n",
       "            3.7043e-04, -1.3200e-04],\n",
       "          [-3.6844e-05, -5.4157e-04,  2.5454e-04,  ..., -1.7415e-04,\n",
       "            3.5932e-04, -1.1321e-04],\n",
       "          [-2.7331e-04, -4.4654e-04,  1.6115e-04,  ..., -1.6344e-04,\n",
       "            5.3620e-04, -2.0818e-04]]], grad_fn=<UnsafeViewBackward0>),\n",
       " tensor(2.9803, grad_fn=<NllLossBackward0>),\n",
       " tensor(0.1400))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3d5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
