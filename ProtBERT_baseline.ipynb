{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4ec425",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonghyunlee1993\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035dfe541d1c4109837419eb0835c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666924808329592, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20221021_083904-158dwvkv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jonghyunlee1993/DistilledProtBert/runs/158dwvkv\" target=\"_blank\">ProtBert_lr-3e-05_prot_1024</a></strong> to <a href=\"https://wandb.ai/jonghyunlee1993/DistilledProtBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568363\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NAME = \"ProtBert\"\n",
    "LEARNING_RATE = 3e-5\n",
    "PROT_MAX_LEN = 1024\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, AutoModel, BertConfig, BertModel, BertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name=f'{PROJECT_NAME}_lr-{LEARNING_RATE}_prot_{PROT_MAX_LEN}',\n",
    "                           project='DistilledProtBert')\n",
    "\n",
    "# prot_seq = pd.read_csv(\"data/mol_trans/protein_sequences.csv\")\n",
    "with open(\"data/fasta_list.pkl\", \"rb\") as f:\n",
    "    fasta_list = pickle.load(f)\n",
    "\n",
    "print(len(fasta_list))\n",
    "    \n",
    "train_data, test_data = train_test_split(fasta_list, test_size=0.1, random_state=42, shuffle=True)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=5000, random_state=42, shuffle=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model = BertForMaskedLM.from_pretrained(\"yarongef/DistilProtBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7471e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def encode(self, seq):\n",
    "        return self.tokenizer(\" \".join(seq), max_length=self.max_len, truncation=True)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encode(self.data[idx])\n",
    "    \n",
    "    \n",
    "def collate_batch(batch):\n",
    "    out = []\n",
    "    for b in batch:\n",
    "        out.append(b)\n",
    "        \n",
    "    return tokenizer.pad(out, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "data_sampler = RandomSampler(train_data, replacement=True, num_samples=100000)\n",
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.3)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=150, collate_fn=mlm_collator,\n",
    "                              num_workers=16, pin_memory=True, prefetch_factor=2, \n",
    "                              drop_last=True, sampler=data_sampler)\n",
    "\n",
    "valid_dataset = CustomDataset(valid_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=150, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=2, collate_fn=mlm_collator)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=150, num_workers=16, \n",
    "                             pin_memory=True, prefetch_factor=2, collate_fn=mlm_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f54505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=[1])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[1])` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class DistilledBERT(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        for param in self.model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def step(self, batch):\n",
    "        out = self.model(**batch)\n",
    "        logits, loss = out['logits'], out['loss']\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits, dim=-1), dim=-1)\n",
    "        label = batch['labels']\n",
    "        masked_index = label.gt(0)\n",
    "        \n",
    "        acc = accuracy(torch.masked_select(pred, masked_index), torch.masked_select(label, masked_index))\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.step(batch)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.step(batch)\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self.step(batch)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=3, dirpath=f'weights/{PROJECT_NAME}', filename='DTI-{epoch:03d}-{valid_loss:.4f}-{valid_acc:.4f}'),\n",
    "]\n",
    "\n",
    "predictor = DistilledBERT(model)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, gpus=[1], enable_progress_bar=True, \n",
    "                     callbacks=callbacks, logger=wandb_logger, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5468346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f4c4449fd24723ae0fe1550f93f02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "     test_acc_epoch         0.34653547406196594\n",
      "        test_loss            2.154139995574951\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.154139995574951, 'test_acc_epoch': 0.34653547406196594}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(predictor, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5813be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
