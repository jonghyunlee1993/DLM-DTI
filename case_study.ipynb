{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc4cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tdc.multi_pred import DTI\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "molecule_tokenizer = molecule_tokenizer = BertTokenizer.from_pretrained(\"data/drug/molecule_tokenizer\", model_max_length=128)\n",
    "protein_tokenizer = BertTokenizer.from_pretrained(\"data/target/protein_tokenizer\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f16f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at weights/protein_bert and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class DualLanguageModelDTI(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_encoder, protein_encoder, hidden_dim=512,\n",
    "                 molecule_input_dim=128, protein_input_dim=1024):\n",
    "        super().__init__()\n",
    "        self.molecule_encoder = molecule_encoder\n",
    "        self.protein_encoder = protein_encoder\n",
    "        \n",
    "        # model freezing without last layer\n",
    "        for param in self.molecule_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False        \n",
    "        for param in self.protein_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.molecule_align = nn.Sequential(\n",
    "            nn.LayerNorm(molecule_input_dim),\n",
    "            nn.Linear(molecule_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.protein_align = nn.Sequential(\n",
    "            nn.LayerNorm(protein_input_dim),\n",
    "            nn.Linear(protein_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, molecule_seq, protein_seq):\n",
    "        encoded_molecule = self.molecule_encoder(**molecule_seq)\n",
    "        encoded_protein = self.protein_encoder(**protein_seq)\n",
    "        \n",
    "        cls_molecule = encoded_molecule.pooler_output\n",
    "        cls_protein = encoded_protein.pooler_output\n",
    "        \n",
    "        cls_molecule = self.molecule_align(cls_molecule)\n",
    "        cls_protein = self.protein_align(cls_protein)\n",
    "        \n",
    "        cls_concat = torch.cat([cls_molecule, cls_protein], dim=1)\n",
    "\n",
    "        x = F.gelu(self.fc1(cls_concat))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = F.gelu(self.fc3(x))\n",
    "        out = self.fc_out(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert\")\n",
    "protein_bert = BertModel.from_pretrained(\"weights/protein_bert\")\n",
    "dlm_dti = DualLanguageModelDTI(molecule_bert, protein_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba3fd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, dlm_dti):\n",
    "        super().__init__()\n",
    "        self.model = dlm_dti\n",
    "\n",
    "        \n",
    "    def forward(self, molecule_sequence, protein_sequence):\n",
    "        return self.model(molecule_sequence, protein_sequence)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    \n",
    "dti_regressor = DTI_prediction(dlm_dti)\n",
    "ckpt_fname = \"dlm_dti-epoch=194-valid_loss=0.2036-valid_mae=0.2368.ckpt\"\n",
    "dti_regressor = dti_regressor.load_from_checkpoint(\"weights/dlm_dti_davis_512/\" + ckpt_fname, dlm_dti=dlm_dti)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator=\"cpu\", enable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69b162cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def encode_sequences(molecule, protein):\n",
    "    canonical_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(molecule))\n",
    "    \n",
    "    molecule = molecule_tokenizer(\" \".join(canonical_smiles), \n",
    "                                  max_length=100, truncation=True,\n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "    protein = protein_tokenizer(\" \".join(protein), \n",
    "                                max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "    \n",
    "    return molecule, protein\n",
    "\n",
    "\n",
    "def predict(molecule, protein, dti_regressor):\n",
    "    molecule, protein = encode_sequences(molecule, protein)\n",
    "    \n",
    "    dti_regressor.model.eval()\n",
    "    pred = dti_regressor.model(molecule, protein)  \n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83709384",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "To log space...\n"
     ]
    }
   ],
   "source": [
    "davis = DTI(name=\"davis\")\n",
    "davis.convert_to_log(form = 'binding')\n",
    "davis_split = davis.get_split()\n",
    "\n",
    "train_df = davis_split['train']\n",
    "valid_df = davis_split['valid']\n",
    "test_df = davis_split['test']\n",
    "# test_df[test_df.Y >= 7].loc[:, [\"Target_ID\", \"Target\"]].drop_duplicates().head(30).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8dfb09e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.6080]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[5.1738]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[7.1522]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[5.0940]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[7.1733]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[5.0296]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[8.4587]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[5.0094]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[5.0660]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# VEGFR2, https://go.drugbank.com/bio_entities/BE0000369\n",
    "VEGFR2 = \"MQSKVLLAVALWLCVETRAASVGLPSVSLDLPRLSIQKDILTIKANTTLQITCRGQRDLDWLWPNNQSGSEQRVEVTECSDGLFCKTLTIPKVIGNDTGAYKCFYRETDLASVIYVYVQDYRSPFIASVSDQHGVVYITENKNKTVVIPCLGSISNLNVSLCARYPEKRFVPDGNRISWDSKKGFTIPSYMISYAGMVFCEAKINDESYQSIMYIVVVVGYRIYDVVLSPSHGIELSVGEKLVLNCTARTELNVGIDFNWEYPSSKHQHKKLVNRDLKTQSGSEMKKFLSTLTIDGVTRSDQGLYTCAASSGLMTKKNSTFVRVHEKPFVAFGSGMESLVEATVGERVRIPAKYLGYPPPEIKWYKNGIPLESNHTIKAGHVLTIMEVSERDTGNYTVILTNPISKEKQSHVVSLVVYVPPQIGEKSLISPVDSYQYGTTQTLTCTVYAIPPPHHIHWYWQLEEECANEPSQAVSVTNPYPCEEWRSVEDFQGGNKIEVNKNQFALIEGKNKTVSTLVIQAANVSALYKCEAVNKVGRGERVISFHVTRGPEITLQPDMQPTEQESVSLWCTADRSTFENLTWYKLGPQPLPIHVGELPTPVCKNLDTLWKLNATMFSNSTNDILIMELKNASLQDQGDYVCLAQDRKTKKRHCVVRQLTVLERVAPTITGNLENQTTSIGESIEVSCTASGNPPPQIMWFKDNETLVEDSGIVLKDGNRNLTIRRVRKEDEGLYTCQACSVLGCAKVEAFFIIEGAQEKTNLEIIILVGTAVIAMFFWLLLVIILRTVKRANGGELKTGYLSIVMDPDELPLDEHCERLPYDASKWEFPRDRLKLGKPLGRGAFGQVIEADAFGIDKTATCRTVAVKMLKEGATHSEHRALMSELKILIHIGHHLNVVNLLGACTKPGGPLMVIVEFCKFGNLSTYLRSKRNEFVPYKTKGARFRQGKDYVGAIPVDLKRRLDSITSSQSSASSGFVEEKSLSDVEEEEAPEDLYKDFLTLEHLICYSFQVAKGMEFLASRKCIHRDLAARNILLSEKNVVKICDFGLARDIYKDPDYVRKGDARLPLKWMAPETIFDRVYTIQSDVWSFGVLLWEIFSLGASPYPGVKIDEEFCRRLKEGTRMRAPDYTTPEMYQTMLDCWHGEPSQRPTFSELVEHLGNLLQANAQQDGKDYIVLPISETLSMEEDSGLSLPTSPVSCMEEEEVCDPKFHYDNTAGISQYLQNSKRKSRPVSVKTFEDIPLEEPEVKVIPDDNQTDSGMVLASEELKTLEDRTKLSPSFGGMVPSKSRESVASEGSNQTSGYQSGYHSDDTDTTVYSSEEAELLKLIEIGVQTGSTAQILQPDSGTTLSSPPV\"\n",
    "sunitinib = \"CCN(CC)CCNC(=O)C1=C(C)NC(\\C=C2/C(=O)NC3=C2C=C(F)C=C3)=C1C\"\n",
    "midostaurin = \"CO[C@@H]1[C@@H](C[C@H]2O[C@]1(C)N1C3=C(C=CC=C3)C3=C1C1=C(C4=C(C=CC=C4)N21)C1=C3CNC1=O)N(C)C(=O)C1=CC=CC=C1\"\n",
    "axitinib = \"CNC(=O)C1=C(SC2=CC=C3C(NN=C3\\C=C\\C3=CC=CC=N3)=C2)C=CC=C1\"\n",
    "cabozantinib = \"COC1=CC2=C(C=C1OC)C(OC1=CC=C(NC(=O)C3(CC3)C(=O)NC3=CC=C(F)C=C3)C=C1)=CC=N2\"\n",
    "regorafenib = \"CNC(=O)C1=CC(OC2=CC(F)=C(NC(=O)NC3=CC=C(Cl)C(=C3)C(F)(F)F)C=C2)=CC=N1\"\n",
    "lenvatinib = \"COC1=C(C=C2C(OC3=CC=C(NC(=O)NC4CC4)C(Cl)=C3)=CC=NC2=C1)C(N)=O\"\n",
    "nintendanib = \"COC(=O)C1=CC=C2C(NC(=O)\\C2=C(/NC2=CC=C(C=C2)N(C)C(=O)CN2CCN(C)CC2)C2=CC=CC=C2)=C1\"\n",
    "ripretinib = \"CCN1C(=O)C(=CC2=C1C=C(NC)N=C2)C1=C(Br)C=C(F)C(NC(=O)NC2=CC=CC=C2)=C1\"\n",
    "tivozanib = \"COC1=C(OC)C=C2C(OC3=CC(Cl)=C(NC(=O)NC4=NOC(C)=C4)C=C3)=CC=NC2=C1\"\n",
    "\n",
    "predict(sunitinib, VEGFR2, dti_regressor)\n",
    "predict(midostaurin, VEGFR2, dti_regressor)\n",
    "predict(axitinib, VEGFR2, dti_regressor)\n",
    "predict(cabozantinib, VEGFR2, dti_regressor)\n",
    "predict(regorafenib, VEGFR2, dti_regressor)\n",
    "predict(lenvatinib, VEGFR2, dti_regressor)\n",
    "predict(nintendanib, VEGFR2, dti_regressor)\n",
    "predict(ripretinib, VEGFR2, dti_regressor)\n",
    "predict(tivozanib, VEGFR2, dti_regressor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
