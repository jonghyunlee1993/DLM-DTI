{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20404929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "molecule_tokenizer = molecule_tokenizer = BertTokenizer.from_pretrained(\"data/drug/molecule_tokenizer\", model_max_length=128)\n",
    "protein_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc55274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at weights/molecule_bert and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at weights/protein_bert and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim=128, intermediate_dim=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        project_out = input_dim\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = (input_dim / heads) ** -0.5\n",
    "\n",
    "        self.key = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.value = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.query = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(intermediate_dim, project_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        b, n, d, h = *data.shape, self.heads\n",
    "\n",
    "        k = self.key(data)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        v = self.value(data)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
    "        \n",
    "        # get only cls token\n",
    "        q = self.query(data[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attention = dots.softmax(dim=-1)\n",
    "\n",
    "        output = einsum('b h i j, b h j d -> b h i d', attention, v)\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "    \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_dim=128, molecule_intermediate_dim=256,\n",
    "                 protein_dim=1024, protein_intermediate_dim=2048,\n",
    "                 cross_attn_depth=1, cross_attn_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cross_attn_layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(cross_attn_depth):\n",
    "            self.cross_attn_layers.append(nn.ModuleList([\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                PreNorm(protein_dim, CrossAttention(\n",
    "                    protein_dim, protein_intermediate_dim, cross_attn_heads, dropout\n",
    "                )),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                PreNorm(molecule_dim, CrossAttention(\n",
    "                    molecule_dim, molecule_intermediate_dim, cross_attn_heads, dropout\n",
    "                ))\n",
    "            ]))\n",
    "\n",
    "            \n",
    "    def forward(self, molecule, protein):\n",
    "        for i, (f_sl, g_ls, cross_attn_s, f_ls, g_sl, cross_attn_l) in enumerate(self.cross_attn_layers):\n",
    "            \n",
    "            cls_molecule = molecule[:, 0]\n",
    "            x_molecule = molecule[:, 1:]\n",
    "            \n",
    "            cls_protein = protein[:, 0]\n",
    "            x_protein = protein[:, 1:]\n",
    "\n",
    "            # Cross attention for protein sequence\n",
    "            cal_q = f_ls(cls_protein.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_molecule), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_l(cal_qkv)\n",
    "            cal_out = F.gelu(g_sl(cal_out))\n",
    "            protein = torch.cat((cal_out, x_protein), dim=1)\n",
    "\n",
    "            # Cross attention for molecule sequence\n",
    "            cal_q = f_sl(cls_molecule.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_protein), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_s(cal_qkv)\n",
    "            cal_out = F.gelu(g_ls(cal_out))\n",
    "            molecule = torch.cat((cal_out, x_molecule), dim=1)\n",
    "            \n",
    "        return molecule, protein\n",
    "    \n",
    "    \n",
    "class AttentionalDTI(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_encoder, protein_encoder, cross_attention_layer, \n",
    "                 molecule_input_dim=128, protein_input_dim=1024, hidden_dim=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.molecule_encoder = molecule_encoder\n",
    "        self.protein_encoder = protein_encoder\n",
    "        \n",
    "        # model freezing without last layer\n",
    "        for param in self.molecule_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False        \n",
    "        for param in self.protein_encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.cross_attention_layer = cross_attention_layer\n",
    "        \n",
    "        self.molecule_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(molecule_input_dim),\n",
    "            nn.Linear(molecule_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.protein_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(protein_input_dim),\n",
    "            nn.Linear(protein_input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, molecule_seq, protein_seq):\n",
    "        encoded_molecule = self.molecule_encoder(**molecule_seq)\n",
    "        encoded_protein = self.protein_encoder(**protein_seq)\n",
    "        \n",
    "        molecule_out, protein_out = self.cross_attention_layer(encoded_molecule.last_hidden_state, encoded_protein.last_hidden_state)\n",
    "        \n",
    "        molecule_out = molecule_out[:, 0]\n",
    "        protein_out = protein_out[:, 0]\n",
    "        \n",
    "        # cls token\n",
    "        molecule_projected = self.molecule_mlp(molecule_out)\n",
    "        protein_projected = self.protein_mlp(protein_out)\n",
    "        \n",
    "        out = self.fc_out(molecule_projected + protein_projected)\n",
    "        \n",
    "        return out\n",
    "\n",
    "molecule_bert = BertModel.from_pretrained(\"weights/molecule_bert\")\n",
    "protein_bert = BertModel.from_pretrained(\"weights/protein_bert\")\n",
    "cross_attention_layer = CrossAttentionLayer()\n",
    "attentional_dti = AttentionalDTI(molecule_bert, protein_bert, cross_attention_layer, cross_attn_depth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4747092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, attentional_dti):\n",
    "        super().__init__()\n",
    "        self.model = attentional_dti\n",
    "\n",
    "        \n",
    "    def forward(self, molecule_sequence, protein_sequence):\n",
    "        return self.model(molecule_sequence, protein_sequence)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        \n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_mae\", mean_absolute_error(y_hat, y), on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        molecule_sequence, protein_sequence, y = batch\n",
    "        \n",
    "        y_hat = self(molecule_sequence, protein_sequence).squeeze(-1)        \n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "\n",
    "model = DTI_prediction(attentional_dti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2700b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_fname = \"attentional_dti-epoch=049-valid_loss=0.1788-valid_mae=0.2497.ckpt\"\n",
    "\n",
    "model = model.load_from_checkpoint(\"weights/Attentional_DTI_cross_attention_kiba/\" + ckpt_fname, attentional_dti=attentional_dti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a4df4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_encode(molecule_sequence):\n",
    "    molecule_sequence = molecule_tokenizer(\n",
    "        \" \".join(molecule_sequence), \n",
    "        max_length=100, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return molecule_sequence\n",
    "\n",
    "\n",
    "def protein_encode(protein_sequence):\n",
    "    protein_sequence = protein_tokenizer(\n",
    "        \" \".join(protein_sequence), \n",
    "        max_length=512, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return protein_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d58c52",
   "metadata": {},
   "source": [
    "# PDE5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9972442",
   "metadata": {},
   "outputs": [],
   "source": [
    "pde5 = \"MATALNHVSREEVEKYLEANHDVATDIFVTKATPDMIDQWLSKHANSLHKHGEGSPQDVSSWPDVSMKLTEKGVFQSIRKSFNISGTKSLRNLLSPRRRKSTLKRNKSALRQLDEKELFMELIRDIADELDLNTLCHKILMNVSILTNGDRCSLFLARGTKDRRFLVSKLFDVNENSTVEDSLHSEEEEIHIPFGQGIAGHVAQTKETVNIKNAYEDKRFNPEVDKITGYKTHSIMCMPICNHDGEVVGVAQVINKITGSHEFAAKDEEAQVELRRIVSHEFNPADEEVFKNYLTFCGIGIMNAQLFEMSVNEYKRNQMLLQLARGIFEEQTSLDNVVHKIMRQAVSLLKCQRCMVFILETTEESYLPAQLRMAEGKRHSIAYQSSFDAPLNDVKNISFLKGFELTDEDTEKLKTIPHEMLKNSINATIARHVADSGETTNIADFTVQKQFKEISDVDPEFRIRSVLCQPIYNSEQKIIGVAQMINKACKQTFTDQDEHLFEAFAIFCGLGIHNTQMFENAMRLMAKQQVALDVLSYHATAQPDEVSKLKKSCVPSARELKLYEFSFSDFDLTEDQTLQGTLRMFIECNLIEKYHIPYDVLCRWTLSVRKNYRPVIYHNWRHAFNVAQTMFSIVMTGKLRKLLTDLEIFALIVACLCHDLDHRGTNNTFQVKTSSPLSLLYGTSTMEHHHFDHCIMILNSEGNNIFEFMSPDDYREAIRMLESAILSTDLAIYFKKRADFFKLVEKGEHTWDNEEKKGLLRGMLMTACDVSAIAKPWLVQQKVAELVFSEFFQQGDLEREKLKEEPMAMMDRKKKDELPKMQVGFIDGICMPVYKMFAELWPDLKPLESGTQLNRDNWQALSEGKEPNDWGSSPPSLQTSKQMESTILQNDRTQLDTLDEKPSLECIQKQEGSRSTGGGEPKKRGSQMSQQCKEALAAKKNKSSLCSVI\"\n",
    "\n",
    "sildenafil = \"CCCC1=NN(C2=C1N=C(NC2=O)C3=C(C=CC(=C3)S(=O)(=O)N4CCN(CC4)C)OCC)C\"\n",
    "tadalafil = \"CN1CC(=O)N2C(C1=O)CC3=C(C2C4=CC5=C(C=C4)OCO5)NC6=CC=CC=C36\"\n",
    "vardenafil = \"CCCC1=NC(=C2N1N=C(NC2=O)C3=C(C=CC(=C3)S(=O)(=O)N4CCN(CC4)CC)OCC)C\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "952eaae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "949"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pde5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f576349",
   "metadata": {},
   "outputs": [],
   "source": [
    "pde5_ = protein_encode(pde5)\n",
    "\n",
    "sildenafil_ = molecule_encode(sildenafil)\n",
    "tadalafil_ = molecule_encode(tadalafil)\n",
    "vardenafil_ = molecule_encode(vardenafil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "312fa003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sildenafil_ tensor([[12.8052]], grad_fn=<AddmmBackward0>)\n",
      "tadalafil_ tensor([[13.5117]], grad_fn=<AddmmBackward0>)\n",
      "vardenafil_ tensor([[12.4900]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mol_ = [\"sildenafil_\", \"tadalafil_\", \"vardenafil_\"]\n",
    "\n",
    "model.model.eval()\n",
    "for i, mol in enumerate([sildenafil_, tadalafil_, vardenafil_]):\n",
    "    res = model.model(mol, pde5_)\n",
    "    print(mol_[i], res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf5727",
   "metadata": {},
   "source": [
    "# HMG-CoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7af98478",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmg_coa = \"MLSRLFRMHGLFVASHPWEVIVGTVTLTICMMSMNMFTGNNKICGWNYECPKFEEDVLSSDIIILTITRCIAILYIYFQFQNLRQLGSKYILGIAGLFTIFSSFVFSTVVIHFLDKELTGLNEALPFFLLLIDLSRASTLAKFALSSNSQDEVRENIARGMAILGPTFTLDALVECLVIGVGTMSGVRQLEIMCCFGCMSVLANYFVFMTFFPACVSLVLELSRESREGRPIWQLSHFARVLEEEENKPNPVTQRVKMIMSLGLVLVHAHSRWIADPSPQNSTADTSKVSLGLDENVSKRIEPSVSLWQFYLSKMISMDIEQVITLSLALLLAVKYIFFEQTETESTLSLKNPITSPVVTQKKVPDNCCRREPMLVRNNQKCDSVEEETGINRERKVEVIKPLVAETDTPNRATFVVGNSSLLDTSSVLVTQEPEIELPREPRPNEECLQILGNAEKGAKFLSDAEIIQLVNAKHIPAYKLETLMETHERGVSIRRQLLSKKLSEPSSLQYLPYRDYNYSLVMGACCENVIGYMPIPVGVAGPLCLDEKEFQVPMATTEGCLVASTNRGCRAIGLGGGASSRVLADGMTRGPVVRLPRACDSAEVKAWLETSEGFAVIKEAFDSTSRFARLQKLHTSIAGRNLYIRFQSRSGDAMGMNMISKGTEKALSKLHEYFPEMQILAVSGNYCTDKKPAAINWIEGRGKSVVCEAVIPAKVVREVLKTTTEAMIEVNINKNLVGSAMAGSIGGYNAHAANIVTAIYIACGQDAAQNVGSSNCITLMEASGPTNEDLYISCTMPSIEIGTVGGGTNLLPQQACLQMLGVQGACKDNPGENARQLARIVCGTVMAGELSLMAALAAGHLVKSHMIHNRSKINLQDLQGACTKKTA\"\n",
    "\n",
    "atorvastatin = \"CC(C)C1=C(C(=C(N1CCC(CC(CC(=O)O)O)O)C2=CC=C(C=C2)F)C3=CC=CC=C3)C(=O)NC4=CC=CC=C4\"\n",
    "lovastatin = \"CCC(C)C(=O)OC1CC(C=C2C1C(C(C=C2)C)CCC3CC(CC(=O)O3)O)C\"\n",
    "pravastatin = \"CCC(C)C(=O)OC1CC(C=C2C1C(C(C=C2)C)CCC(CC(CC(=O)O)O)O)O\"\n",
    "rosuvastatin =\"CC(C)C1=NC(=NC(=C1C=CC(CC(CC(=O)[O-])O)O)C2=CC=C(C=C2)F)N(C)S(=O)(=O)C.CC(C)C1=NC(=NC(=C1C=CC(CC(CC(=O)[O-])O)O)C2=CC=C(C=C2)F)N(C)S(=O)(=O)C.[Ca+2]\"\n",
    "simvastatin = \"CCC(C)(C)C(=O)OC1CC(C=C2C1C(C(C=C2)C)CCC3CC(CC(=O)O3)O)C\"\n",
    "fluvastatin = \"CC(C)N1C2=CC=CC=C2C(=C1C=CC(CC(CC(=O)O)O)O)C3=CC=C(C=C3)F\"\n",
    "pitavastatin = \"C1CC1C2=NC3=CC=CC=C3C(=C2C=CC(CC(CC(=O)O)O)O)C4=CC=C(C=C4)F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5e323ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hmg_coa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5170843",
   "metadata": {},
   "outputs": [],
   "source": [
    "atorvastatin_ = molecule_encode(atorvastatin)\n",
    "lovastatin_ = molecule_encode(lovastatin)\n",
    "pravastatin_ = molecule_encode(pravastatin)\n",
    "rosuvastatin_ = molecule_encode(rosuvastatin)\n",
    "simvastatin_ = molecule_encode(simvastatin)\n",
    "fluvastatin_ = molecule_encode(fluvastatin)\n",
    "pitavastatin_ = molecule_encode(pitavastatin)\n",
    "\n",
    "hmg_coa_ = protein_encode(hmg_coa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97f0a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atorvastatin_ tensor([[12.6351]], grad_fn=<AddmmBackward0>)\n",
      "lovastatin_ tensor([[10.8080]], grad_fn=<AddmmBackward0>)\n",
      "pravastatin_ tensor([[10.6947]], grad_fn=<AddmmBackward0>)\n",
      "rosuvastatin_ tensor([[12.2706]], grad_fn=<AddmmBackward0>)\n",
      "simvastatin_ tensor([[10.7043]], grad_fn=<AddmmBackward0>)\n",
      "fluvastatin_ tensor([[11.6072]], grad_fn=<AddmmBackward0>)\n",
      "pitavastatin_ tensor([[11.5024]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mol_ = [\"atorvastatin_\", \"lovastatin_\", \"pravastatin_\", \"rosuvastatin_\", \"simvastatin_\", \"fluvastatin_\", \"pitavastatin_\"]\n",
    "\n",
    "model.model.eval()\n",
    "for i, mol in enumerate([atorvastatin_, lovastatin_, pravastatin_, rosuvastatin_, simvastatin_, fluvastatin_, pitavastatin_]):\n",
    "    res = model.model(mol, hmg_coa_)\n",
    "    print(mol_[i], res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a96152",
   "metadata": {},
   "source": [
    "# MLKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "656d2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlkl = \"GSPGENLKHIITLGQVIHKRCEEMKYCKKQCRRLGHRVLGLIKPLEMLQDQGKRSVPSEKLTTAMNRFKAALEEANGEIEKFSNRSNICRFLTASQDKILFKDVNRKLSDVWKELSLLLQVEQRMPVSPISQGASWAQEDQQDADEDRRAFQMLRRD\"\n",
    "\n",
    "necrosulfonamide = \"COC1=NC=CN=C1NS(=O)(=O)C2=CC=C(C=C2)NC(=O)C=CC3=CC=C(S3)[N+](=O)[O-]\"\n",
    "TC13172 = \"CN1C2=C(N=C1S(=O)(=O)C)N(C(=O)N(C2=O)C)CC#CC3=CC(=CC=C3)O\"\n",
    "GW806742X = \"CN(C1=CC=C(C=C1)NC(=O)NC2=CC=C(C=C2)OC(F)(F)F)C3=NC(=NC=C3)NC4=CC(=CC=C4)S(=O)(=O)N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d60bfbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ea289da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlkl_ = protein_encode(mlkl)\n",
    "\n",
    "necrosulfonamide_ = molecule_encode(necrosulfonamide)\n",
    "TC13172_ = molecule_encode(TC13172)\n",
    "GW806742X_ = molecule_encode(GW806742X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efa7b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "necrosulfonamide_ tensor([[13.0803]], grad_fn=<AddmmBackward0>)\n",
      "TC13172_ tensor([[12.8356]], grad_fn=<AddmmBackward0>)\n",
      "GW806742X_ tensor([[12.6528]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mol_ = [\"necrosulfonamide_\", \"TC13172_\", \"GW806742X_\"]\n",
    "\n",
    "model.model.eval()\n",
    "for i, mol in enumerate([necrosulfonamide_, TC13172_, GW806742X_]):\n",
    "    res = model.model(mol, mlkl_)\n",
    "    print(mol_[i], res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e2ded",
   "metadata": {},
   "source": [
    "# Cyclooxygenase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1199d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclooxygenase2 = \"MLARALLLCAVLALSHTANPCCSHPCQNRGVCMSVGFDQYKCDCTRTGFYGENCSTPEFLTRIKLFLKPTPNTVHYILTHFKGFWNVVNNIPFLRNAIMSYVLTSRSHLIDSPPTYNADYGYKSWEAFSNLSYYTRALPPVPDDCPTPLGVKGKKQLPDSNEIVGKLLLRRKFIPDPQGSNMMFAFFAQHFTHQFFKTDHKRGPAFTNGLGHGVDLNHIYGETLARQRKLRLFKDGKMKYQIIDGEMYPPTVKDTQAEMIYPPQVPEHLRFAVGQEVFGLVPGLMMYATIWLREHNRVCDVLKQEHPEWGDEQLFQTSRLILIGKQENDLYKTLFPREN\"\n",
    "\n",
    "celecoxib = \"CC1=CC=C(C=C1)C2=CC(=NN2C3=CC=C(C=C3)S(=O)(=O)N)C(F)(F)F\"\n",
    "valdecoxib = \"CC1=C(C(=NO1)C2=CC=CC=C2)C3=CC=C(C=C3)S(=O)(=O)N\"\n",
    "rofecoxib = \"CS(=O)(=O)C1=CC=C(C=C1)C2=C(C(=O)OC2)C3=CC=CC=C3\"\n",
    "etoricoxib = \"CC1=NC=C(C=C1)C2=C(C=C(C=N2)Cl)C3=CC=C(C=C3)S(=O)(=O)C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1ca885c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cyclooxygenase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9079aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclooxygenase2_ = protein_encode(cyclooxygenase2)\n",
    "\n",
    "celecoxib_ = molecule_encode(celecoxib)\n",
    "valdecoxib_ = molecule_encode(valdecoxib)\n",
    "rofecoxib_ = molecule_encode(rofecoxib)\n",
    "etoricoxib_ = molecule_encode(etoricoxib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b594174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celecoxib_ tensor([[12.6371]], grad_fn=<AddmmBackward0>)\n",
      "valdecoxib_ tensor([[12.3927]], grad_fn=<AddmmBackward0>)\n",
      "rofecoxib_ tensor([[11.3644]], grad_fn=<AddmmBackward0>)\n",
      "etoricoxib_ tensor([[12.1555]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mol_ = [\"celecoxib_\", \"valdecoxib_\", \"rofecoxib_\", \"etoricoxib_\"]\n",
    "\n",
    "model.model.eval()\n",
    "for i, mol in enumerate([celecoxib_, valdecoxib_, rofecoxib_, etoricoxib_]):\n",
    "    res = model.model(mol, cyclooxygenase2_)\n",
    "    print(mol_[i], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b11e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
