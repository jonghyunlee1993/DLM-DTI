{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151c57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Bio import SeqIO\n",
    "\n",
    "# fasta_file = SeqIO.parse(open(\"data/uniprot_sprot.fasta\"), 'fasta')\n",
    "# fasta_list = []\n",
    "\n",
    "# for fasta in fasta_file:\n",
    "#     fasta_list.append(str(fasta.seq))\n",
    "    \n",
    "# import pickle \n",
    "\n",
    "# with open(\"data/fasta_list.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fasta_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4ec425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NAME = \"XProtBert\"\n",
    "LEARNING_RATE = 1e-4\n",
    "PROT_MAX_LEN = 1024\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, AutoModel, BertConfig, BertModel, BertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name=f'{PROJECT_NAME}_lr-{LEARNING_RATE}_prot_{PROT_MAX_LEN}',\n",
    "                           project='DistilledProtBert')\n",
    "\n",
    "# prot_seq = pd.read_csv(\"data/mol_trans/protein_sequences.csv\")\n",
    "with open(\"data/fasta_list.pkl\", \"rb\") as f:\n",
    "    fasta_list = pickle.load(f)\n",
    "\n",
    "print(len(fasta_list))\n",
    "    \n",
    "train_data, test_data = train_test_split(fasta_list, test_size=0.1, random_state=42, shuffle=True)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=5000, random_state=42, shuffle=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "teacher_model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=1024,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=PROT_MAX_LEN + 2,\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "student_model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7471e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def encode(self, seq):\n",
    "        return self.tokenizer(\" \".join(seq), max_length=self.max_len, truncation=True)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encode(self.data[idx])\n",
    "    \n",
    "    \n",
    "def collate_batch(batch):\n",
    "    out = []\n",
    "    for b in batch:\n",
    "        out.append(b)\n",
    "        \n",
    "    return tokenizer.pad(out, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "data_sampler = RandomSampler(train_data, replacement=True, num_samples=100000)\n",
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.3)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=mlm_collator,\n",
    "                              num_workers=16, pin_memory=True, prefetch_factor=5, \n",
    "                              drop_last=True, sampler=data_sampler)\n",
    "\n",
    "valid_dataset = CustomDataset(valid_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=5, collate_fn=mlm_collator)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, tokenizer, max_len=PROT_MAX_LEN)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=16, \n",
    "                             pin_memory=True, prefetch_factor=5, collate_fn=mlm_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f54505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class DistilledBERT(pl.LightningModule):\n",
    "    def __init__(self, teacher_model, student_model):\n",
    "        super().__init__()\n",
    "        self.teacher_model = teacher_model\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.teacher_model.eval()\n",
    "        \n",
    "        self.student_model = student_model\n",
    "        self.proj = nn.Linear(512, 1024, bias=False)\n",
    "        \n",
    "    \n",
    "    def step(self, batch):\n",
    "        teacher_hidden = teacher_model.bert(input_ids=batch['input_ids'], \n",
    "                   attention_mask=batch['attention_mask'],\n",
    "                   token_type_ids=batch['token_type_ids'])['last_hidden_state']\n",
    "        \n",
    "        student_hidden = student_model.bert(input_ids=batch['input_ids'], \n",
    "                   attention_mask=batch['attention_mask'],\n",
    "                   token_type_ids=batch['token_type_ids'])['last_hidden_state']\n",
    "        student_hidden_proj = self.proj(student_hidden)\n",
    "        \n",
    "        student_logits = student_model.cls(student_hidden)\n",
    "        student_loss = F.cross_entropy(student_logits.reshape(-1, 30), batch['labels'].reshape(-1))\n",
    "        \n",
    "        logit_loss = F.mse_loss(student_hidden_proj, teacher_hidden.detach())\n",
    "        \n",
    "        total_loss = (student_loss + logit_loss)/2\n",
    "        \n",
    "        pred = torch.argmax(F.softmax(student_logits, dim=-1), dim=-1)\n",
    "        acc = accuracy(torch.masked_select(pred, batch['labels'].gt(0)), torch.masked_select(batch['labels'], batch['labels'].gt(0)))\n",
    "        \n",
    "        return total_loss, acc\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.step(batch)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        student_out = student_model(**batch)\n",
    "        student_loss, student_logits = student_out['loss'], student_out['logits']\n",
    "        \n",
    "        pred = torch.argmax(F.softmax(student_out['logits'], dim=-1), dim=-1)\n",
    "        label = batch['labels']\n",
    "        acc = accuracy(torch.masked_select(pred, label.gt(0)), torch.masked_select(label, label.gt(0)))\n",
    "        \n",
    "        self.log('valid_loss', student_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        student_out = student_model(**batch)\n",
    "        student_loss, student_logits = student_out['loss'], student_out['logits']\n",
    "        \n",
    "        pred = torch.argmax(F.softmax(student_out['logits'], dim=-1), dim=-1)\n",
    "        label = batch['labels']\n",
    "        acc = accuracy(torch.masked_select(pred, label.gt(0)), torch.masked_select(label, label.gt(0)))\n",
    "        \n",
    "        self.log('test_loss', student_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=3, dirpath=f'weights/{PROJECT_NAME}', filename='DTI-{epoch:03d}-{valid_loss:.4f}-{valid_acc:.4f}'),\n",
    "]\n",
    "\n",
    "predictor = DistilledBERT(teacher_model, student_model)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=[0], enable_progress_bar=True, \n",
    "                     callbacks=callbacks, logger=wandb_logger, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a3e04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /home/ubuntu/Workspace/DLM_DTI/weights/XProtBert exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type            | Params\n",
      "--------------------------------------------------\n",
      "0 | teacher_model | BertForMaskedLM | 419 M \n",
      "1 | student_model | BertForMaskedLM | 26.0 M\n",
      "2 | proj          | Linear          | 524 K \n",
      "--------------------------------------------------\n",
      "26.6 M    Trainable params\n",
      "419 M     Non-trainable params\n",
      "446 M     Total params\n",
      "892.994   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d4aeece26417ab2e35799d6efd5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(predictor, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12049eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predictor.load_from_checkpoint(\"weights/DistilledProtBert/DTI-epoch=002-valid_loss=0.2429-valid_acc=0.9101.ckpt\",\n",
    "#     teacher_model=teacher_model, student_model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "046d59a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:489: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef514d97ea1416bad782c490bdc1f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.31578874588012695\n",
      "        test_loss           1.6354262828826904\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.6354262828826904, 'test_acc': 0.31578874588012695}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(predictor, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e0f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edc8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc23c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "946690b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predictor.student_model.state_dict(), \"weights/XProtBert/XProtBERT_150K_update.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dcc2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.student_model.model.save_pretrained(\"weights/XProtBert/XProtBERT_mlm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432dfbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
