{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abeefeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonghyunlee1993\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20221025_002447-20xkne3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jonghyunlee1993/DLM_DTI/runs/20xkne3d\" target=\"_blank\">CrossViT_MSE_half_freeze_lr-1e-05_prot_545</a></strong> to <a href=\"https://wandb.ai/jonghyunlee1993/DLM_DTI\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NAME = \"CrossViT_MSE_half_freeze\"\n",
    "LEARNING_RATE = 1e-5\n",
    "PROT_MAX_LEN = 545\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.functional import average_precision\n",
    "from torchmetrics.functional.classification import binary_auroc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name=f'{PROJECT_NAME}_lr-{LEARNING_RATE}_prot_{PROT_MAX_LEN}',\n",
    "                           project='DLM_DTI')\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizer, RobertaTokenizer\n",
    "\n",
    "train_data = pd.read_csv(\"data/mol_trans/train_dataset.csv\")\n",
    "valid_data = pd.read_csv(\"data/mol_trans/valid_dataset.csv\")\n",
    "test_data = pd.read_csv(\"data/mol_trans/test_dataset.csv\")\n",
    "    \n",
    "mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "mol_encoder = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_encoder = AutoModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "for param in prot_encoder.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in prot_encoder.encoder.layer[:16]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81a04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        labels=None,\n",
    "        indices=None,\n",
    "        num_samples=None,\n",
    "        callback_get_label=None,\n",
    "    ):\n",
    "        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = dataset.data.Label\n",
    "\n",
    "        # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Label\"] = self._get_labels(dataset) if labels is None else labels\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"Label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"Label\"]]\n",
    "\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, mol_tokenizer, prot_tokenizer):\n",
    "        self.data = data\n",
    "        self.mol_tokenizer = mol_tokenizer\n",
    "        self.prot_tokenizer = prot_tokenizer\n",
    "        \n",
    "    def get_mol_feature(self, smiles):\n",
    "        return self.mol_tokenizer(smiles, max_length=512, truncation=True)\n",
    "    \n",
    "    def get_prot_feature(self, fasta):\n",
    "        return self.prot_tokenizer(\" \".join(fasta), max_length=PROT_MAX_LEN, truncation=True)\n",
    "    \n",
    "    def __len__(self):    \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        smiles = self.data.loc[index, \"SMILES\"]\n",
    "        mol_feature = self.get_mol_feature(smiles)\n",
    "        \n",
    "        fasta = self.data.loc[index, \"Target Sequence\"]\n",
    "        prot_feature = self.get_prot_feature(fasta)\n",
    "        \n",
    "        y = self.data.loc[index, \"Label\"]\n",
    "        source = self.data.loc[index, \"Source\"]\n",
    "                \n",
    "        return mol_feature, prot_feature, y, source\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    mol_features, prot_features, y, source = [], [], [], []\n",
    "    \n",
    "    for (mol_seq, prot_seq, y_, source_) in batch:\n",
    "        mol_features.append(mol_seq)\n",
    "        prot_features.append(prot_seq)\n",
    "        y.append(y_)\n",
    "        source.append(source_)\n",
    "        \n",
    "    mol_features = mol_tokenizer.pad(mol_features, return_tensors=\"pt\")\n",
    "    prot_features = prot_tokenizer.pad(prot_features, return_tensors=\"pt\")\n",
    "    y = torch.tensor(y).float()\n",
    "    source = torch.tensor(source)\n",
    "    \n",
    "    return mol_features, prot_features, y, source\n",
    "\n",
    "\n",
    "train_dataset = DTIDataset(train_data, mol_tokenizer, prot_tokenizer)\n",
    "valid_dataset = DTIDataset(valid_data, mol_tokenizer, prot_tokenizer)\n",
    "test_dataset = DTIDataset(test_data, mol_tokenizer, prot_tokenizer)\n",
    "\n",
    "\n",
    "train_dataset = DTIDataset(train_data, mol_tokenizer, prot_tokenizer)\n",
    "valid_dataset = DTIDataset(valid_data, mol_tokenizer, prot_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=10, drop_last=True, \n",
    "                              sampler=ImbalancedDatasetSampler(train_dataset, labels=train_dataset.data.Label),\n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=10,\n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=16, \n",
    "                             pin_memory=True, prefetch_factor=10,\n",
    "                             collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f997ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim=128, intermediate_dim=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        project_out = input_dim\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = (input_dim / heads) ** -0.5\n",
    "\n",
    "        self.key = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.value = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "        self.query = nn.Linear(input_dim, intermediate_dim, bias=False)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(intermediate_dim, project_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        b, n, d, h = *data.shape, self.heads\n",
    "\n",
    "        k = self.key(data)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        v = self.value(data)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
    "        \n",
    "        # get only cls token\n",
    "        q = self.query(data[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attention = dots.softmax(dim=-1)\n",
    "\n",
    "        output = einsum('b h i j, b h j d -> b h i d', attention, v)\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "    \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 molecule_dim=768, molecule_intermediate_dim=1024,\n",
    "                 protein_dim=1024, protein_intermediate_dim=2048,\n",
    "                 cross_attn_depth=1, cross_attn_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cross_attn_layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(cross_attn_depth):\n",
    "            self.cross_attn_layers.append(nn.ModuleList([\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                PreNorm(protein_dim, CrossAttention(\n",
    "                    protein_dim, protein_intermediate_dim, cross_attn_heads, dropout\n",
    "                )),\n",
    "                nn.Linear(protein_dim, molecule_dim),\n",
    "                nn.Linear(molecule_dim, protein_dim),\n",
    "                PreNorm(molecule_dim, CrossAttention(\n",
    "                    molecule_dim, molecule_intermediate_dim, cross_attn_heads, dropout\n",
    "                ))\n",
    "            ]))\n",
    "\n",
    "            \n",
    "    def forward(self, molecule, protein):\n",
    "        for i, (f_sl, g_ls, cross_attn_s, f_ls, g_sl, cross_attn_l) in enumerate(self.cross_attn_layers):\n",
    "            \n",
    "            cls_molecule = molecule[:, 0]\n",
    "            x_molecule = molecule[:, 1:]\n",
    "            \n",
    "            cls_protein = protein[:, 0]\n",
    "            x_protein = protein[:, 1:]\n",
    "\n",
    "            # Cross attention for protein sequence\n",
    "            cal_q = f_ls(cls_protein.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_molecule), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_l(cal_qkv)\n",
    "            cal_out = F.gelu(g_sl(cal_out))\n",
    "            protein = torch.cat((cal_out, x_protein), dim=1)\n",
    "\n",
    "            # Cross attention for molecule sequence\n",
    "            cal_q = f_sl(cls_molecule.unsqueeze(1))\n",
    "            cal_qkv = torch.cat((cal_q, x_protein), dim=1)\n",
    "            # add activation function\n",
    "            cal_out = cal_q + cross_attn_s(cal_qkv)\n",
    "            cal_out = F.gelu(g_ls(cal_out))\n",
    "            molecule = torch.cat((cal_out, x_molecule), dim=1)\n",
    "            \n",
    "        return molecule, protein\n",
    "    \n",
    "\n",
    "class DTI(nn.Module):\n",
    "    def __init__(self, mol_encoder, prot_encoder, \n",
    "                 hidden_dim=512, mol_dim=128, prot_dim=1024):\n",
    "        super().__init__()\n",
    "        self.mol_encoder = mol_encoder\n",
    "        self.prot_encoder = prot_encoder\n",
    "        \n",
    "        self.cross_attention = CrossAttentionLayer(cross_attn_depth=2, cross_attn_heads=4)\n",
    "        \n",
    "        self.molecule_align = nn.Sequential(\n",
    "            nn.LayerNorm(mol_dim),\n",
    "            nn.Linear(mol_dim, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.protein_align = nn.Sequential(\n",
    "            nn.LayerNorm(prot_dim),\n",
    "            nn.Linear(prot_dim, hidden_dim, bias=False)\n",
    "        )       \n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.cls_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, SMILES, FASTA):\n",
    "        mol_feature = self.mol_encoder(**SMILES).last_hidden_state\n",
    "        prot_feature = self.prot_encoder(**FASTA).last_hidden_state\n",
    "        \n",
    "        mol_feature, prot_feature = self.cross_attention(mol_feature, prot_feature)\n",
    "        mol_feature = mol_feature[:, 0]\n",
    "        prot_feature = prot_feature[:, 0]\n",
    "        \n",
    "        mol_feature = self.molecule_align(mol_feature)\n",
    "        prot_feature = self.protein_align(prot_feature)\n",
    "        \n",
    "        x = torch.cat([mol_feature, prot_feature], dim=1)\n",
    "\n",
    "        x = F.dropout(F.gelu(self.fc1(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc2(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc3(x)), 0.1)\n",
    "        \n",
    "        cls_out = self.cls_out(x).squeeze(-1)\n",
    "        \n",
    "        return F.tanh(cls_out)\n",
    "    \n",
    "model = DTI(mol_encoder, prot_encoder,\n",
    "            hidden_dim=512, mol_dim=768, prot_dim=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c092692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=[1])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[1])` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    \n",
    "    def step(self, batch):\n",
    "        mol_feature, prot_feature, y, source = batch\n",
    "        pred = self.model(mol_feature, prot_feature).squeeze(-1)\n",
    "        \n",
    "#         loss = F.binary_cross_entropy_with_logits(pred, y)\n",
    "        loss = F.smooth_l1_loss(pred, y)\n",
    "    \n",
    "        auroc = binary_auroc(pred, y)\n",
    "        auprc = average_precision(pred, y)\n",
    "        \n",
    "        return pred, source, loss, auroc, auprc, \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('train_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('valid_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('test_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        pred, source, _, _, _ = self.step(batch)\n",
    "        \n",
    "        return pred, batch[2], source\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "          \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_auroc', mode=\"max\",\n",
    "                    save_top_k=5, dirpath=f'weights/{PROJECT_NAME}', filename='DTI-{epoch:03d}-{valid_loss:.4f}-{valid_auroc:.4f}-{valid_auprc:.4f}'),\n",
    "]\n",
    "\n",
    "predictor = DTI_prediction(model)\n",
    "trainer = pl.Trainer(max_epochs=200, gpus=[1], enable_progress_bar=True, \n",
    "                     callbacks=callbacks, logger=wandb_logger, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614af2f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | DTI  | 499 M \n",
      "-------------------------------\n",
      "256 M     Trainable params\n",
      "242 M     Non-trainable params\n",
      "499 M     Total params\n",
      "998.114   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2301b79b51e4070af6d8985f97c7801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de16624f8464ff0ac7a0910bef472bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(predictor, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ab9a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad746d35b5040759b13740d564839c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor = predictor.load_from_checkpoint(\n",
    "    \"weights/CrossViT_MSE//DTI-epoch=036-valid_loss=0.0703-valid_auroc=0.9005-valid_auprc=nan.ckpt\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "pred_out = trainer.predict(predictor, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22254160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>y</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.977051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.346191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.996094</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  y  source\n",
       "0  0.000000  0       0\n",
       "1  0.000000  0       0\n",
       "2  0.977051  0       0\n",
       "3  0.346191  0       0\n",
       "4  0.996094  1       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.array([]).reshape(0, 3)\n",
    "\n",
    "for pred, label, source in pred_out:\n",
    "    line_ = np.array([pred.detach().numpy(), \n",
    "         label.detach().numpy(), \n",
    "         source.detach().numpy()]).T\n",
    "    results = np.vstack([results, line_])\n",
    "\n",
    "results = pd.DataFrame(results, columns=[\"pred\", \"y\", \"source\"])\n",
    "results.y = results.y.astype(int)\n",
    "results.source = results.source.astype(int)\n",
    "results.loc[results.pred < 0, \"pred\"] = 0\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4626d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Davis\n",
      "AUROC\tAUPRC\n",
      "0.8962\t0.3446\n",
      "\n",
      "Dataset: BindingDB\n",
      "AUROC\tAUPRC\n",
      "0.884\t0.8712\n",
      "\n",
      "Dataset: BIOSNAP\n",
      "AUROC\tAUPRC\n",
      "0.9077\t0.5844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation_metrics(df, source=0):\n",
    "    from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "    \n",
    "    def get_cindex(Y, P):\n",
    "        summ = 0\n",
    "        pair = 0\n",
    "\n",
    "        for i in range(1, len(Y)):\n",
    "            for j in range(0, i):\n",
    "                if i is not j:\n",
    "                    if(Y[i] > Y[j]):\n",
    "                        pair +=1\n",
    "                        summ +=  1* (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n",
    "\n",
    "        if pair is not 0:\n",
    "            return summ/pair\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def r_squared_error(y_obs,y_pred):\n",
    "        y_obs = np.array(y_obs)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
    "        y_pred_mean = [np.mean(y_pred) for y in y_pred]\n",
    "\n",
    "        mult = sum((y_pred - y_pred_mean) * (y_obs - y_obs_mean))\n",
    "        mult = mult * mult\n",
    "\n",
    "        y_obs_sq = sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
    "        y_pred_sq = sum((y_pred - y_pred_mean) * (y_pred - y_pred_mean) )\n",
    "\n",
    "        return mult / float(y_obs_sq * y_pred_sq)\n",
    "\n",
    "    def get_k(y_obs,y_pred):\n",
    "        y_obs = np.array(y_obs)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        return sum(y_obs*y_pred) / float(sum(y_pred*y_pred))\n",
    "\n",
    "    def squared_error_zero(y_obs,y_pred):\n",
    "        k = get_k(y_obs,y_pred)\n",
    "\n",
    "        y_obs = np.array(y_obs)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
    "        upp = sum((y_obs - (k*y_pred)) * (y_obs - (k* y_pred)))\n",
    "        down= sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
    "\n",
    "        return 1 - (upp / float(down))\n",
    "\n",
    "\n",
    "    def get_rm2(ys_orig, ys_line):\n",
    "        r2 = r_squared_error(ys_orig, ys_line)\n",
    "        r02 = squared_error_zero(ys_orig, ys_line)\n",
    "\n",
    "        return r2 * (1 - np.sqrt(np.absolute((r2*r2)-(r02*r02))))\n",
    "    \n",
    "    \n",
    "    source_df = df[df.source == source].reset_index(drop=True)\n",
    "    auroc = roc_auc_score(source_df.y, source_df.pred)\n",
    "    auprc = average_precision_score(source_df.y, source_df.pred)\n",
    "\n",
    "    \n",
    "    if source == 0:\n",
    "        dataset = \"Davis\"\n",
    "    elif source == 1:\n",
    "        dataset = \"BindingDB\"\n",
    "    elif source == 2:\n",
    "        dataset = \"BIOSNAP\"\n",
    "        \n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(\"AUROC\\tAUPRC\")\n",
    "    print(f\"{auroc.round(4)}\\t{auprc.round(4)}\")\n",
    "    print()\n",
    "    \n",
    "get_evaluation_metrics(results, source=0)\n",
    "get_evaluation_metrics(results, source=1)\n",
    "get_evaluation_metrics(results, source=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46bace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
