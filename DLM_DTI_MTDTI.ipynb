{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c018897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonghyunlee1993\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20221024_073031-nhz24dmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jonghyunlee1993/DLM_DTI/runs/nhz24dmw\" target=\"_blank\">MTDTI_MSE_lr-5e-05_prot_1024</a></strong> to <a href=\"https://wandb.ai/jonghyunlee1993/DLM_DTI\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROJECT_NAME = \"MTDTI_MSE\"\n",
    "LEARNING_RATE = 5e-5\n",
    "PROT_MAX_LEN = 1024\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.functional import average_precision\n",
    "from torchmetrics.functional.classification import binary_auroc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name=f'{PROJECT_NAME}_lr-{LEARNING_RATE}_prot_{PROT_MAX_LEN}',\n",
    "                           project='DLM_DTI')\n",
    "\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "\n",
    "train_data = pd.read_csv(\"data/mol_trans/train_dataset.csv\")\n",
    "valid_data = pd.read_csv(\"data/mol_trans/valid_dataset.csv\")\n",
    "test_data = pd.read_csv(\"data/mol_trans/test_dataset.csv\")\n",
    "    \n",
    "mol_tokenizer = BertTokenizer.from_pretrained(\"jonghyunlee/DrugLikeMoleculeBERT\")\n",
    "mol_encoder = AutoModel.from_pretrained(\"jonghyunlee/DrugLikeMoleculeBERT\")\n",
    "\n",
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4539d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        labels=None,\n",
    "        indices=None,\n",
    "        num_samples=None,\n",
    "        callback_get_label=None,\n",
    "    ):\n",
    "        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = dataset.data.Label\n",
    "\n",
    "        # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Label\"] = self._get_labels(dataset) if labels is None else labels\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"Label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"Label\"]]\n",
    "\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, mol_tokenizer, prot_tokenizer):\n",
    "        self.data = data\n",
    "        self.mol_tokenizer = mol_tokenizer\n",
    "        self.prot_tokenizer = prot_tokenizer\n",
    "        \n",
    "    def get_mol_feature(self, smiles):\n",
    "        return self.mol_tokenizer(\" \".join(smiles), max_length=128, truncation=True)\n",
    "    \n",
    "    def get_prot_feature(self, fasta):\n",
    "        return self.prot_tokenizer(\" \".join(fasta), max_length=PROT_MAX_LEN, truncation=True)\n",
    "    \n",
    "    def __len__(self):    \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        smiles = self.data.loc[index, \"SMILES\"]\n",
    "        mol_feature = self.get_mol_feature(smiles)\n",
    "        \n",
    "        fasta = self.data.loc[index, \"Target Sequence\"]\n",
    "        prot_feature = self.get_prot_feature(fasta)\n",
    "        \n",
    "        y = self.data.loc[index, \"Label\"]\n",
    "        source = self.data.loc[index, \"Source\"]\n",
    "                \n",
    "        return mol_feature, prot_feature, y, source\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    mol_features, prot_features, y, source = [], [], [], []\n",
    "    \n",
    "    for (mol_seq, prot_seq, y_, source_) in batch:\n",
    "        mol_features.append(mol_seq)\n",
    "        prot_features.append(prot_seq)\n",
    "        y.append(y_)\n",
    "        source.append(source_)\n",
    "        \n",
    "    mol_features = mol_tokenizer.pad(mol_features, return_tensors=\"pt\")\n",
    "    prot_features = prot_tokenizer.pad(prot_features, return_tensors=\"pt\")\n",
    "    y = torch.tensor(y).float()\n",
    "    source = torch.tensor(source)\n",
    "    \n",
    "    return mol_features, prot_features['input_ids'], y, source\n",
    "\n",
    "\n",
    "train_dataset = DTIDataset(train_data, mol_tokenizer, prot_tokenizer)\n",
    "valid_dataset = DTIDataset(valid_data, mol_tokenizer, prot_tokenizer)\n",
    "test_dataset = DTIDataset(test_data, mol_tokenizer, prot_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=10, drop_last=True, \n",
    "                              sampler=ImbalancedDatasetSampler(train_dataset, labels=train_dataset.data.Label),\n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=10,\n",
    "                              collate_fn=collate_batch)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=16, \n",
    "                             pin_memory=True, prefetch_factor=10,\n",
    "                             collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29167c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_dim, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x.long())\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, kernel_size, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_dim, embedding_dim, dropout_rate)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=kernel_size[0], padding=1)       \n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=kernel_size[1], padding=1)        \n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=kernel_size[2], padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embedding(x)\n",
    "        x = x.moveaxis(1, 2)\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.adaptive_max_pool1d(x, output_size=1)\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DTI(nn.Module):\n",
    "    def __init__(self, mol_encoder, prot_vocab_dim, embedding_dim, \n",
    "                 hidden_dim=512, dropout_rate=0.1):\n",
    "        super(DTI, self).__init__()\n",
    "        prot_encoder_kernel_size = [4, 8, 12]\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.mol_encoder = mol_encoder\n",
    "        self.prot_encoder = Encoder(prot_vocab_dim, embedding_dim, prot_encoder_kernel_size, dropout_rate=dropout_rate)\n",
    "\n",
    "        self.mol_align = nn.Sequential(\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Linear(128, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.prot_align = nn.Sequential(\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, hidden_dim, bias=False)\n",
    "        )       \n",
    "        \n",
    "        self.fc1 = nn.Linear(2*hidden_dim, 2*hidden_dim)\n",
    "        self.fc2 = nn.Linear(2*hidden_dim, 2*hidden_dim)\n",
    "        self.fc3 = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, SMILES, target):\n",
    "        mol_feature = self.mol_encoder(**SMILES).pooler_output\n",
    "        mol_feature = self.mol_align(mol_feature)\n",
    "        \n",
    "        prot_feature = self.prot_encoder(target)\n",
    "        prot_feature = self.prot_align(prot_feature)\n",
    "        \n",
    "        x = torch.cat((mol_feature, prot_feature), axis=1)\n",
    "\n",
    "        x = F.dropout(F.relu(self.fc1(x)), self.dropout_rate)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), self.dropout_rate)\n",
    "        x = F.dropout(F.relu(self.fc3(x)), self.dropout_rate)\n",
    "        \n",
    "        out = self.out(x)\n",
    "        \n",
    "        return F.tanh(out)\n",
    "    \n",
    "    \n",
    "model = DTI(mol_encoder, prot_tokenizer.vocab_size, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d0067e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    \n",
    "    def step(self, batch):\n",
    "        mol_feature, prot_feature, y, source = batch\n",
    "        pred = self.model(mol_feature, prot_feature).squeeze(-1)\n",
    "        \n",
    "#         loss = F.binary_cross_entropy_with_logits(pred, y)\n",
    "        loss = F.smooth_l1_loss(pred, y)\n",
    "    \n",
    "        auroc = binary_auroc(pred, y)\n",
    "        auprc = average_precision(pred, y)\n",
    "        \n",
    "        return pred, source, loss, auroc, auprc, \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('train_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('valid_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('test_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        pred, source, _, _, _ = self.step(batch)\n",
    "        \n",
    "        return pred, batch[2], source\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_auroc', mode=\"max\",\n",
    "                    save_top_k=5, dirpath=f'weights/{PROJECT_NAME}', filename='DTI-{epoch:03d}-{valid_loss:.4f}-{valid_auroc:.4f}-{valid_auprc:.4f}'),\n",
    "]\n",
    "\n",
    "predictor = DTI_prediction(model)\n",
    "trainer = pl.Trainer(max_epochs=100, gpus=[0], enable_progress_bar=True, \n",
    "                     callbacks=callbacks, logger=wandb_logger, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32223021",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer.fit(predictor, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7707e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82761b7f57cb4097a7d02a51ac5427bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor = predictor.load_from_checkpoint(\n",
    "    \"weights/MTDTI_MSE/DTI-epoch=099-valid_loss=0.0724-valid_auroc=0.9168-valid_auprc=0.6605.ckpt\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "pred_out = trainer.predict(predictor, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e35b21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>y</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012749</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.181885</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.906738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.662109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  y  source\n",
       "0  0.012749  0       0\n",
       "1  0.181885  0       0\n",
       "2  0.906738  0       0\n",
       "3  0.662109  0       0\n",
       "4  1.000000  1       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.array([]).reshape(0, 3)\n",
    "\n",
    "for pred, label, source in pred_out:\n",
    "    line_ = np.array([pred.detach().numpy(), \n",
    "         label.detach().numpy(), \n",
    "         source.detach().numpy()]).T\n",
    "    results = np.vstack([results, line_])\n",
    "\n",
    "results = pd.DataFrame(results, columns=[\"pred\", \"y\", \"source\"])\n",
    "results.y = results.y.astype(int)\n",
    "results.source = results.source.astype(int)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1371b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Davis\n",
      "AUROC\tAUPRC\tCindex\trm2\n",
      "0.9276\t0.3886\t0.9212\t0.1898\n",
      "\n",
      "Dataset: BindingDB\n",
      "AUROC\tAUPRC\tCindex\trm2\n",
      "0.8831\t0.8807\t0.8869\t0.3718\n",
      "\n",
      "Dataset: BIOSNAP\n",
      "AUROC\tAUPRC\tCindex\trm2\n",
      "0.9134\t0.5933\t0.913\t0.3417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_evaluation_metrics(df, source=0):\n",
    "    from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "    \n",
    "    def get_cindex(Y, P):\n",
    "        summ = 0\n",
    "        pair = 0\n",
    "\n",
    "        for i in range(1, len(Y)):\n",
    "            for j in range(0, i):\n",
    "                if i is not j:\n",
    "                    if(Y[i] > Y[j]):\n",
    "                        pair +=1\n",
    "                        summ +=  1* (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n",
    "\n",
    "        if pair is not 0:\n",
    "            return summ/pair\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def r_squared_error(y_obs,y_pred):\n",
    "        y_obs = np.array(y_obs)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
    "        y_pred_mean = [np.mean(y_pred) for y in y_pred]\n",
    "\n",
    "        mult = sum((y_pred - y_pred_mean) * (y_obs - y_obs_mean))\n",
    "        mult = mult * mult\n",
    "\n",
    "        y_obs_sq = sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
    "        y_pred_sq = sum((y_pred - y_pred_mean) * (y_pred - y_pred_mean) )\n",
    "\n",
    "        return mult / float(y_obs_sq * y_pred_sq)\n",
    "\n",
    "    def get_k(y_obs,y_pred):\n",
    "        y_obs = np.array(y_obs)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        return sum(y_obs*y_pred) / float(sum(y_pred*y_pred))\n",
    "\n",
    "    def squared_error_zero(y_obs,y_pred):\n",
    "        k = get_k(y_obs,y_pred)\n",
    "\n",
    "        y_obs = np.array(y_obs)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
    "        upp = sum((y_obs - (k*y_pred)) * (y_obs - (k* y_pred)))\n",
    "        down= sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
    "\n",
    "        return 1 - (upp / float(down))\n",
    "\n",
    "\n",
    "    def get_rm2(ys_orig, ys_line):\n",
    "        r2 = r_squared_error(ys_orig, ys_line)\n",
    "        r02 = squared_error_zero(ys_orig, ys_line)\n",
    "\n",
    "        return r2 * (1 - np.sqrt(np.absolute((r2*r2)-(r02*r02))))\n",
    "    \n",
    "    \n",
    "    source_df = df[df.source == source].reset_index(drop=True)\n",
    "    auroc = roc_auc_score(source_df.y, source_df.pred)\n",
    "    auprc = average_precision_score(source_df.y, source_df.pred)\n",
    "    \n",
    "    cindex = get_cindex(source_df.y, source_df.pred)\n",
    "    rm2 = get_rm2(source_df.y, source_df.pred)\n",
    "\n",
    "    \n",
    "    if source == 0:\n",
    "        dataset = \"Davis\"\n",
    "    elif source == 1:\n",
    "        dataset = \"BindingDB\"\n",
    "    elif source == 2:\n",
    "        dataset = \"BIOSNAP\"\n",
    "        \n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(\"AUROC\\tAUPRC\\tCindex\\trm2\")\n",
    "    print(f\"{auroc.round(4)}\\t{auprc.round(4)}\\t{cindex.round(4)}\\t{rm2.round(4)}\")\n",
    "    print()\n",
    "    \n",
    "get_evaluation_metrics(results, source=0)\n",
    "get_evaluation_metrics(results, source=1)\n",
    "get_evaluation_metrics(results, source=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c3e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
