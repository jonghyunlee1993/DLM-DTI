{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e84f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tdc.multi_pred import DTI\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "mol_tokenizer = AutoTokenizer.from_pretrained(\"jonghyunlee/DrugLikeMoleculeBERT\")\n",
    "mol_encoder = AutoModel.from_pretrained(\"jonghyunlee/DrugLikeMoleculeBERT\")\n",
    "mol_encoder.to(\"cuda\")\n",
    "mol_encoder.eval()\n",
    "\n",
    "prot_tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_encoder = AutoModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "prot_encoder.to(\"cuda\")\n",
    "prot_encoder.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7a191c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def get_unique(df):\n",
    "    try:\n",
    "        mols = df.get_data().loc[:, [\"Drug_ID\", \"Drug\"]].drop_duplicates().reset_index(drop=True)\n",
    "        prots = df.get_data().loc[:, [\"Target_ID\", \"Target\"]].drop_duplicates().reset_index(drop=True)\n",
    "    except:\n",
    "        mols = df.loc[:, [\"Drug_ID\", \"Drug\"]].drop_duplicates().reset_index(drop=True)\n",
    "        prots = df.loc[:, [\"Target_ID\", \"Target\"]].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return mols, prots\n",
    "\n",
    "\n",
    "davis = DTI(name=\"davis\")\n",
    "davis_mols, davis_prots = get_unique(davis)\n",
    "\n",
    "kiba = DTI(name=\"kiba\")\n",
    "kiba_mols, kiba_prots = get_unique(kiba)\n",
    "\n",
    "biosnap = pd.read_csv(\"data/BIOSNAP.csv\")\n",
    "biosnap_mols, biosnap_prots = get_unique(biosnap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38e05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "To log space...\n"
     ]
    }
   ],
   "source": [
    "binding = DTI(name=\"BindingDB_Kd\")\n",
    "binding.convert_to_log(form=\"binding\")\n",
    "binding = binding.get_data()\n",
    "\n",
    "def generate_prot_dict():\n",
    "    with open(\"data/BindingDB_prot_ID.txt\", \"r\") as f:\n",
    "        prot_id = f.readlines()\n",
    "\n",
    "    prot_dict = {}\n",
    "    for i in range(0, len(prot_id), 2):\n",
    "        meta_line = prot_id[i].rstrip()\n",
    "        prot_name = \"_\".join(meta_line.split(\" \")[3:])\n",
    "        fasta = prot_id[i+1].rstrip()\n",
    "        prot_dict[fasta] = prot_name\n",
    "        \n",
    "    return prot_dict\n",
    "    \n",
    "prot_dict = generate_prot_dict()\n",
    "\n",
    "for i, line in binding.iterrows():\n",
    "    binding.loc[i, \"Target_ID\"] = prot_dict[line.Target]\n",
    "    \n",
    "binding.to_csv(\"data/BindingDB_new_ID.csv\", index=False)\n",
    "binding_mols, binding_prots = get_unique(binding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79479111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davis_mols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 68/68 [00:01<00:00, 44.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiba_mols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2068/2068 [00:21<00:00, 98.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binding_mols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10661/10661 [01:38<00:00, 108.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biosnap_mols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4510/4510 [00:37<00:00, 118.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davis_prots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 379/379 [00:30<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiba_prots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 229/229 [00:18<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binding_prots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1413/1413 [01:57<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biosnap_prots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2182/2182 [03:03<00:00, 11.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(df, fname, mode=\"mol\"):\n",
    "    cls_feature_dict = {}\n",
    "    full_feature_dict = {}\n",
    "    \n",
    "    print(fname)\n",
    "    for i, data in tqdm(df.iterrows(), total=len(df)):\n",
    "        name, seq = data[0], data[1]\n",
    "\n",
    "        if mode == \"mol\":\n",
    "            X = mol_tokenizer.encode_plus(\" \".join(seq) + \" [PAD]\" * (128-len(seq)), \n",
    "                                          return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "            output = mol_encoder(**X.to(\"cuda\"))\n",
    "        elif mode == \"prot\":\n",
    "            X = prot_tokenizer.encode_plus(\" \".join(seq) + \" [PAD]\" * (1024-len(seq)), \n",
    "                                           return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "            output = prot_encoder(**X.to(\"cuda\"))\n",
    "        \n",
    "        cls_feature_dict[name] = output[1].detach().to(\"cpu\")\n",
    "        full_feature_dict[name] = output[0].detach().to(\"cpu\")\n",
    "    \n",
    "    with open(\"data/\" + fname + \"_cls.pkl\", \"wb\") as f:\n",
    "        pickle.dump(cls_feature_dict, f)\n",
    "    \n",
    "    with open(\"data/\" + fname + \"_full.pkl\", \"wb\") as f:\n",
    "        pickle.dump(full_feature_dict, f)\n",
    "        \n",
    "        \n",
    "get_embeddings(davis_mols, \"davis_mols\", \"mol\")\n",
    "get_embeddings(kiba_mols, \"kiba_mols\", \"mol\")\n",
    "get_embeddings(binding_mols, \"binding_mols\", \"mol\")\n",
    "get_embeddings(biosnap_mols, \"biosnap_mols\", \"mol\")\n",
    "\n",
    "get_embeddings(davis_prots, \"davis_prots\", \"prot\")\n",
    "get_embeddings(kiba_prots, \"kiba_prots\", \"prot\")\n",
    "get_embeddings(binding_prots, \"binding_prots\", \"prot\")\n",
    "get_embeddings(biosnap_prots, \"biosnap_prots\", \"prot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6818a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def generate_merged_dict(flist, fname):\n",
    "    merged_dict = {}\n",
    "    \n",
    "    for file_name in flist:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            sub_dict = pickle.load(f)\n",
    "            \n",
    "        merged_dict.update(sub_dict)\n",
    "        \n",
    "    with open(\"data/\" + fname + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(merged_dict, f)\n",
    "\n",
    "# mols_cls_list = glob(\"data/*_mols_cls.pkl\")\n",
    "mols_full_list = glob(\"data/*_mols_full.pkl\")\n",
    "# prots_cls_list = glob(\"data/*_prots_cls.pkl\")\n",
    "prots_full_list = glob(\"data/*_prots_full.pkl\")\n",
    "\n",
    "# generate_merged_dict(mol_cls_list, \"mols_cls\")\n",
    "generate_merged_dict(mols_full_list, \"mols_full\")\n",
    "# generate_merged_dict(prots_cls_list, \"prots_cls\")\n",
    "generate_merged_dict(prots_full_list, \"prots_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ecab9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "To log space...\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(dataset_name):\n",
    "    if dataset_name == \"davis\":\n",
    "        data = DTI(name=dataset_name)\n",
    "        data.convert_to_log(form=\"binding\")\n",
    "        df = data.get_data()\n",
    "        df.loc[:, \"Y_label\"] = df.Y.map(lambda x: 1 if x >= 7 else 0)\n",
    "        df.loc[:, \"Source\"] = \"DAVIS\"\n",
    "        df.loc[:, \"Source_ID\"] = 0\n",
    "    elif dataset_name == \"BindingDB\":\n",
    "        df = pd.read_csv(\"data/BindingDB_new_ID.csv\")\n",
    "        df.loc[:, \"Y_label\"] = df.Y.map(lambda x: 1 if x >= 7 else 0)\n",
    "        df.loc[:, \"Source\"] = \"BindingDB\"\n",
    "        df.loc[:, \"Source_ID\"] = 1\n",
    "    elif dataset_name == \"kiba\":\n",
    "        df = DTI(name=dataset_name).get_data()\n",
    "        df.loc[:, \"Y_label\"] = df.Y.map(lambda x: 1 if x >= 12.1 else 0)\n",
    "        df.loc[:, \"Source\"] = \"KIBA\"\n",
    "        df.loc[:, \"Source_ID\"] = 2\n",
    "    else:\n",
    "        df = pd.read_csv(\"data/BIOSNAP.csv\")\n",
    "        df.loc[:, \"Source\"] = \"BIOSNAP\"\n",
    "        df.loc[:, \"Source_ID\"] = 3\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "davis_df = preprocessing(\"davis\")\n",
    "binding_df = preprocessing(\"BindingDB\")\n",
    "kiba_df = preprocessing(\"kiba\")\n",
    "biosnap_df = preprocessing(\"BIOSNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba789c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "davis_index = []\n",
    "binding_index = []\n",
    "kiba_index = []\n",
    "biosnap_index = []\n",
    "\n",
    "fold = StratifiedKFold()\n",
    "for train_index, test_index in fold.split(davis_df, davis_df.Y_label):\n",
    "    davis_index.append([train_index, test_index])\n",
    "\n",
    "for train_index, test_index in fold.split(binding_df, binding_df.Y_label):\n",
    "    binding_index.append([train_index, test_index])\n",
    "    \n",
    "for train_index, test_index in fold.split(kiba_df, kiba_df.Y_label):\n",
    "    kiba_index.append([train_index, test_index])\n",
    "    \n",
    "for train_index, test_index in fold.split(biosnap_df, biosnap_df.Y_label):\n",
    "    biosnap_index.append([train_index, test_index])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1b0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_fold in range(5):\n",
    "    fold_train_df = pd.DataFrame([])\n",
    "    fold_test_df = pd.DataFrame([])\n",
    "\n",
    "    fold_train_df = fold_train_df.append(davis_df.loc[davis_index[n_fold][0]])\n",
    "    fold_test_df = fold_test_df.append(davis_df.loc[davis_index[n_fold][1]])\n",
    "    \n",
    "    fold_train_df = fold_train_df.append(binding_df.loc[binding_index[n_fold][0]])\n",
    "    fold_test_df = fold_test_df.append(binding_df.loc[binding_index[n_fold][1]])\n",
    "    \n",
    "    fold_train_df = fold_train_df.append(kiba_df.loc[kiba_index[n_fold][0]])\n",
    "    fold_test_df = fold_test_df.append(kiba_df.loc[kiba_index[n_fold][1]])\n",
    "    \n",
    "    fold_train_df = fold_train_df.append(biosnap_df.loc[biosnap_index[n_fold][0]])\n",
    "    fold_test_df = fold_test_df.append(biosnap_df.loc[biosnap_index[n_fold][1]])\n",
    "    \n",
    "    fold_train_df = fold_train_df.reset_index(drop=True)\n",
    "    fold_test_df = fold_test_df.reset_index(drop=True)\n",
    "    \n",
    "    with open(f\"data/fold_number_{n_fold}_train.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fold_train_df, f)\n",
    "        \n",
    "    with open(f\"data/fold_number_{n_fold}_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fold_test_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b384b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
