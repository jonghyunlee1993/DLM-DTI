{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abeefeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.functional import average_precision\n",
    "from torchmetrics.functional.classification import binary_auroc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "train_data = pd.read_csv(\"data/mol_trans/train_dataset.csv\")\n",
    "valid_data = pd.read_csv(\"data/mol_trans/valid_dataset.csv\")\n",
    "test_data = pd.read_csv(\"data/mol_trans/test_dataset.csv\")\n",
    "    \n",
    "with open(\"data/mol_trans/mols_full.pkl\", \"rb\") as f:\n",
    "    mols_embedding = pickle.load(f)\n",
    "    \n",
    "with open(\"data/mol_trans/prots_full.pkl\", \"rb\") as f:\n",
    "    prots_embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81a04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        labels=None,\n",
    "        indices=None,\n",
    "        num_samples=None,\n",
    "        callback_get_label=None,\n",
    "    ):\n",
    "        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = dataset.data.Label\n",
    "\n",
    "        # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        df = pd.DataFrame()\n",
    "        df[\"Label\"] = self._get_labels(dataset) if labels is None else labels\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"Label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"Label\"]]\n",
    "\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, data, mols_embedding, prots_embedding):\n",
    "        self.data = data\n",
    "        self.mols_embedding = mols_embedding\n",
    "        self.prots_embedding = prots_embedding\n",
    "        \n",
    "    def get_mol_feature(self, mol_id):\n",
    "        return self.mols_embedding[mol_id]\n",
    "    \n",
    "    def get_prot_feature(self, prot_id):\n",
    "        return self.prots_embedding[prot_id]\n",
    "    \n",
    "    def __len__(self):    \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        mol_id = self.data.loc[index, \"SMILES\"][:10]\n",
    "        mol_feature = self.get_mol_feature(mol_id).squeeze(0)\n",
    "        \n",
    "        prot_id = self.data.loc[index, \"Target Sequence\"][:30]\n",
    "        prot_feature = self.get_prot_feature(prot_id).squeeze(0)\n",
    "        \n",
    "        y = torch.tensor(self.data.loc[index, \"Label\"]).float()\n",
    "                \n",
    "        return mol_feature, prot_feature, y\n",
    "\n",
    "train_dataset = DTIDataset(train_data, mols_embedding, prots_embedding)\n",
    "valid_dataset = DTIDataset(valid_data, mols_embedding, prots_embedding)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=10, drop_last=True, \n",
    "                              sampler=ImbalancedDatasetSampler(train_dataset, labels=train_dataset.data.Label))\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, num_workers=16, \n",
    "                              pin_memory=True, prefetch_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f9ebb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention.\n",
    "    See \"Attention Is All You Need\" for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.,\n",
    "                 bias=True, add_bias_kv=False, add_zero_attn=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.in_proj_weight = nn.Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
    "        self.register_parameter('in_proj_bias', None)\n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.Tensor(3 * embed_dim))\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "            self.bias_v = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.in_proj_bias is not None:\n",
    "            nn.init.constant_(self.in_proj_bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Self-attention can be implemented by passing in the same arguments for\n",
    "        query, key and value. Timesteps can be masked by supplying a T x T mask in the\n",
    "        `attn_mask` argument. Padding elements can be excluded from\n",
    "        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n",
    "        batch x src_len, where padding elements are indicated by 1s.\n",
    "        \"\"\"\n",
    "        qkv_same = query.data_ptr() == key.data_ptr() == value.data_ptr()\n",
    "        kv_same = key.data_ptr() == value.data_ptr()\n",
    "\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "        assert key.size() == value.size()\n",
    "\n",
    "        aved_state = None\n",
    "\n",
    "        if qkv_same:\n",
    "            # self-attention\n",
    "            q, k, v = self.in_proj_qkv(query)\n",
    "        elif kv_same:\n",
    "            # encoder-decoder attention\n",
    "            q = self.in_proj_q(query)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = v = None\n",
    "            else:\n",
    "                k, v = self.in_proj_kv(key)\n",
    "        else:\n",
    "            q = self.in_proj_q(query)\n",
    "            k = self.in_proj_k(key)\n",
    "            v = self.in_proj_v(value)\n",
    "        q = q * self.scaling\n",
    "\n",
    "        if self.bias_k is not None:\n",
    "            assert self.bias_v is not None\n",
    "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
    "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "        \n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            try:\n",
    "                attn_weights += attn_mask.unsqueeze(0)\n",
    "            except:\n",
    "                print(attn_weights.shape)\n",
    "                print(attn_mask.unsqueeze(0).shape)\n",
    "                assert False\n",
    "                \n",
    "        attn_weights = F.softmax(attn_weights.float(), dim=-1).type_as(attn_weights)\n",
    "        # attn_weights = F.relu(attn_weights)\n",
    "        # attn_weights = attn_weights / torch.max(attn_weights)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        attn = torch.bmm(attn_weights, v)\n",
    "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
    "\n",
    "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "\n",
    "        # average attention weights over heads\n",
    "        attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "        attn_weights = attn_weights.sum(dim=1) / self.num_heads\n",
    "        return attn, attn_weights\n",
    "\n",
    "    def in_proj_qkv(self, query):\n",
    "        return self._in_proj(query).chunk(3, dim=-1)\n",
    "\n",
    "    def in_proj_kv(self, key):\n",
    "        return self._in_proj(key, start=self.embed_dim).chunk(2, dim=-1)\n",
    "\n",
    "    def in_proj_q(self, query, **kwargs):\n",
    "        return self._in_proj(query, end=self.embed_dim, **kwargs)\n",
    "\n",
    "    def in_proj_k(self, key):\n",
    "        return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
    "\n",
    "    def in_proj_v(self, value):\n",
    "        return self._in_proj(value, start=2 * self.embed_dim)\n",
    "\n",
    "    def _in_proj(self, input, start=0, end=None, **kwargs):\n",
    "        weight = kwargs.get('weight', self.in_proj_weight)\n",
    "        bias = kwargs.get('bias', self.in_proj_bias)\n",
    "        weight = weight[start:end, :]\n",
    "        if bias is not None:\n",
    "            bias = bias[start:end]\n",
    "        return F.linear(input, weight, bias)\n",
    "    \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n",
    "    is a :class:`TransformerEncoderLayer`.\n",
    "    Args:\n",
    "        embed_tokens (torch.nn.Embedding): input embedding\n",
    "        num_heads (int): number of heads\n",
    "        layers (int): number of layers\n",
    "        attn_dropout (float): dropout applied on the attention weights\n",
    "        relu_dropout (float): dropout applied on the first layer of the residual block\n",
    "        res_dropout (float): dropout applied on the residual block\n",
    "        attn_mask (bool): whether to apply mask on the attention weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, layers, attn_dropout=0.0, relu_dropout=0.0, res_dropout=0.0,\n",
    "                 embed_dropout=0.0, attn_mask=False):\n",
    "        super().__init__()\n",
    "        self.dropout = embed_dropout      # Embedding dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        self.embed_positions = None\n",
    "        \n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for layer in range(layers):\n",
    "            new_layer = TransformerEncoderLayer(embed_dim,\n",
    "                                                num_heads=num_heads,\n",
    "                                                attn_dropout=attn_dropout,\n",
    "                                                relu_dropout=relu_dropout,\n",
    "                                                res_dropout=res_dropout,\n",
    "                                                attn_mask=attn_mask)\n",
    "            self.layers.append(new_layer)\n",
    "\n",
    "        self.register_buffer('version', torch.Tensor([2]))\n",
    "        self.normalize = True\n",
    "        if self.normalize:\n",
    "            self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x_in, x_in_k = None, x_in_v = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_in (FloatTensor): embedded input of shape `(src_len, batch, embed_dim)`\n",
    "            x_in_k (FloatTensor): embedded input of shape `(src_len, batch, embed_dim)`\n",
    "            x_in_v (FloatTensor): embedded input of shape `(src_len, batch, embed_dim)`\n",
    "        Returns:\n",
    "            dict:\n",
    "                - **encoder_out** (Tensor): the last encoder layer's output of\n",
    "                  shape `(src_len, batch, embed_dim)`\n",
    "                - **encoder_padding_mask** (ByteTensor): the positions of\n",
    "                  padding elements of shape `(batch, src_len)`\n",
    "        \"\"\"\n",
    "        # embed tokens and positions\n",
    "        x = self.embed_scale * x_in\n",
    "        if self.embed_positions is not None:\n",
    "            x += self.embed_positions(x_in.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        if x_in_k is not None and x_in_v is not None:\n",
    "            # embed tokens and positions    \n",
    "            x_k = self.embed_scale * x_in_k\n",
    "            x_v = self.embed_scale * x_in_v\n",
    "            if self.embed_positions is not None:\n",
    "                x_k += self.embed_positions(x_in_k.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding\n",
    "                x_v += self.embed_positions(x_in_v.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding\n",
    "            x_k = F.dropout(x_k, p=self.dropout, training=self.training)\n",
    "            x_v = F.dropout(x_v, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # encoder layers\n",
    "        intermediates = [x]\n",
    "        for layer in self.layers:\n",
    "            if x_in_k is not None and x_in_v is not None:\n",
    "                x = layer(x, x_k, x_v)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            intermediates.append(x)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum input length supported by the encoder.\"\"\"\n",
    "        if self.embed_positions is None:\n",
    "            return self.max_source_positions\n",
    "        return min(self.max_source_positions, self.embed_positions.max_positions())\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder layer block.\n",
    "    In the original paper each operation (multi-head attention or FFN) is\n",
    "    postprocessed with: `dropout -> add residual -> layernorm`. In the\n",
    "    tensor2tensor code they suggest that learning is more robust when\n",
    "    preprocessing each layer with layernorm and postprocessing with:\n",
    "    `dropout -> add residual`. We default to the approach in the paper, but the\n",
    "    tensor2tensor approach can be enabled by setting\n",
    "    *args.encoder_normalize_before* to ``True``.\n",
    "    Args:\n",
    "        embed_dim: Embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=4, attn_dropout=0.1, relu_dropout=0.1, res_dropout=0.1,\n",
    "                 attn_mask=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attn = MultiheadAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            attn_dropout=attn_dropout\n",
    "        )\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "        self.relu_dropout = relu_dropout\n",
    "        self.res_dropout = res_dropout\n",
    "        self.normalize_before = True\n",
    "\n",
    "        self.fc1 = nn.Linear(self.embed_dim, 4*self.embed_dim)   # The \"Add & Norm\" part in the paper\n",
    "        self.fc2 = nn.Linear(4*self.embed_dim, self.embed_dim)\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(self.embed_dim) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, x_k=None, x_v=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
    "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
    "            x_k (Tensor): same as x\n",
    "            x_v (Tensor): same as x\n",
    "        Returns:\n",
    "            encoded output of shape `(batch, src_len, embed_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(0, x, before=True)\n",
    "        mask = buffered_future_mask(x, x_k) if self.attn_mask else None\n",
    "        if x_k is None and x_v is None:\n",
    "            x, _ = self.self_attn(query=x, key=x, value=x, attn_mask=mask)\n",
    "        else:\n",
    "            x_k = self.maybe_layer_norm(0, x_k, before=True)\n",
    "            x_v = self.maybe_layer_norm(0, x_v, before=True) \n",
    "            x, _ = self.self_attn(query=x, key=x_k, value=x_v, attn_mask=mask)\n",
    "        x = F.dropout(x, p=self.res_dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(0, x, after=True)\n",
    "\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(1, x, before=True)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.res_dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(1, x, after=True)\n",
    "        return x\n",
    "\n",
    "    def maybe_layer_norm(self, i, x, before=False, after=False):\n",
    "        assert before ^ after\n",
    "        if after ^ self.normalize_before:\n",
    "            return self.layer_norms[i](x)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3697e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTI(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, mol_dim=128, prot_dim=1024, \n",
    "                 nhead=4, nlayer=4, attn_dropout=0.1, relu_dropout=0.1,\n",
    "                 res_dropout=0.1, embed_dropout=0.1, attn_mask=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj_mol = nn.Conv1d(mol_dim, hidden_dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.proj_prot = nn.Conv1d(prot_dim, hidden_dim, kernel_size=1, padding=0, bias=False)\n",
    "        \n",
    "        self.mol2prot = TransformerEncoder(embed_dim=hidden_dim, num_heads=nhead, layers=nlayer,\n",
    "                                           attn_dropout=attn_dropout, relu_dropout=relu_dropout, \n",
    "                                           res_dropout=res_dropout, embed_dropout=embed_dropout, attn_mask=attn_mask)\n",
    "        self.prot2mol = TransformerEncoder(embed_dim=hidden_dim, num_heads=nhead, layers=nlayer,\n",
    "                                           attn_dropout=attn_dropout, relu_dropout=relu_dropout, \n",
    "                                           res_dropout=res_dropout, embed_dropout=embed_dropout, attn_mask=attn_mask)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.cls_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, mol_feature, prot_feature):       \n",
    "        mol_feature = self.proj_mol(mol_feature.transpose(1, 2)).permute(2, 0, 1)\n",
    "        prot_feature = self.proj_prot(prot_feature.transpose(1, 2)).permute(2, 0, 1)\n",
    "        \n",
    "        mol2prot_feature = self.mol2prot(mol_feature, prot_feature, prot_feature)\n",
    "        prot2_mol_feature = self.prot2mol(prot_feature, mol_feature, mol_feature)\n",
    "        \n",
    "        mol_feature = mol2prot_feature.permute(1, 0, 2)\n",
    "        prot_feature = prot2_mol_feature.permute(1, 0, 2)\n",
    "        \n",
    "        mol_feature = mol_feature[:, 0]\n",
    "        prot_feature = prot_feature[:, 0]\n",
    "        \n",
    "        x = torch.cat([mol_feature, prot_feature], dim=1)\n",
    "\n",
    "        x = F.dropout(F.gelu(self.fc1(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc2(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc3(x)), 0.1)\n",
    "        \n",
    "        cls_out = self.cls_out(x).squeeze(-1)\n",
    "        \n",
    "        return cls_out\n",
    "    \n",
    "model = DTI(hidden_dim=1024, mol_dim=128, prot_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c092692",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 63\u001b[0m\n\u001b[1;32m     58\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     59\u001b[0m     ModelCheckpoint(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dirpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/multi_modal_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDTI-\u001b[39m\u001b[38;5;132;01m{epoch:03d}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{valid_loss:.4f}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{valid_auroc:.4f}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{valid_auprc:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     60\u001b[0m ]\n\u001b[1;32m     62\u001b[0m predictor \u001b[38;5;241m=\u001b[39m DTI_prediction(model)\n\u001b[0;32m---> 63\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, gpus\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m], enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py:345\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:433\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:214\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_gpu_accelerator_backend()\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_parallel_devices_and_init_accelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:527\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m AcceleratorRegistry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag)\n\u001b[0;32m--> 527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    528\u001b[0m     available_accelerator \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    529\u001b[0m         acc_str \u001b[38;5;28;01mfor\u001b[39;00m acc_str \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_types \u001b[38;5;28;01mif\u001b[39;00m AcceleratorRegistry\u001b[38;5;241m.\u001b[39mget(acc_str)\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    530\u001b[0m     ]\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m can not run on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_accelerator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/cuda.py:91\u001b[0m, in \u001b[0;36mCUDAAccelerator.is_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdevice_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_cuda_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py:347\u001b[0m, in \u001b[0;36mnum_cuda_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmultiprocessing\u001b[38;5;241m.\u001b[39mget_all_start_methods() \u001b[38;5;129;01mor\u001b[39;00m _is_forking_disabled():\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mapply(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/context.py:119\u001b[0m, in \u001b[0;36mBaseContext.Pool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m'''Returns a process pool object'''\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocesses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxtasksperchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/pool.py:212\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes \u001b[38;5;241m=\u001b[39m processes\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repopulate_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/pool.py:303\u001b[0m, in \u001b[0;36mPool._repopulate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repopulate_pool\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repopulate_pool_static\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inqueue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maxtasksperchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/pool.py:326\u001b[0m, in \u001b[0;36mPool._repopulate_pool_static\u001b[0;34m(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception)\u001b[0m\n\u001b[1;32m    324\u001b[0m w\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcess\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoolWorker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    325\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m pool\u001b[38;5;241m.\u001b[39mappend(w)\n\u001b[1;32m    328\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madded worker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/context.py:277\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_fork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/multiprocessing/popen_fork.py:70\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m parent_r, child_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[1;32m     69\u001b[0m child_r, parent_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "class DTI_prediction(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    \n",
    "    def step(self, batch):\n",
    "        mol_feature, prot_feature, y = batch\n",
    "        pred = self.model(mol_feature, prot_feature)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(pred, y)\n",
    "    \n",
    "        auroc = binary_auroc(pred, y)\n",
    "        auprc = average_precision(pred, y)\n",
    "        \n",
    "        return pred, loss, auroc, auprc\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('train_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('valid_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, loss, auroc, auprc = self.step(batch)\n",
    "        \n",
    "        self.log('test_auroc', auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_auprc', auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        pred, _, _, _ = self.step(batch)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "    \n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=1, dirpath='weights/multi_modal_transformer', filename='DTI-{epoch:03d}-{valid_loss:.4f}-{valid_auroc:.4f}-{valid_auprc:.4f}'),\n",
    "]\n",
    "\n",
    "predictor = DTI_prediction(model)\n",
    "trainer = pl.Trainer(max_epochs=300, gpus=[0], enable_progress_bar=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614af2f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | DTI  | 56.3 M\n",
      "-------------------------------\n",
      "56.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "56.3 M    Total params\n",
      "225.054   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6463d65144498a8eba05a1327c318b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(predictor, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab9a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
